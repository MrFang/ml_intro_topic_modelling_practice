{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f0608b",
   "metadata": {},
   "source": [
    "При подготовке ноутбука использовались материалы:\n",
    "1. [LDA topic modelling lenta Kaggle](https://www.kaggle.com/genyagree/lda-topic-modelling/notebook)\n",
    "2. [LDA topic modelling visualization](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)\n",
    "3. [LDA topic modelling](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06831b6",
   "metadata": {},
   "source": [
    "# Введение\n",
    "**Latent Dirichlet Allocation** (LDA) - статистическая модель, позволяющая разделить набор текстов на $N$ подгрупп.\n",
    "\n",
    "Каждый кластер характеризуется набором из $X$ ключевых слов. Эти ключевые слова ассоциируются с темой (topic). \n",
    "\n",
    "Каждый документ может быть представлен абором тем, каждая тема может быть представлена набором ключевых слов.\n",
    "\n",
    "### Пример\n",
    "У нас есть 3 документа:<br>\n",
    "<br>\n",
    "D1 = \"Собаки любят играть\"<br>\n",
    "D2 = \"Кошки любят молоко\"<br>\n",
    "D3 = \"Кошки и собаки любят кушать и играть. Я люблю собак. Они милые\"<br>\n",
    "\n",
    "Topic Modelling может выдать следующую модель:<br>\n",
    "<br>\n",
    "D1 = 100% Topic1 + 0% Topic2<br>\n",
    "D2 = 0% Topic1 + 100% Topic2<br>\n",
    "D3 = 70% Topic1 + 30% Topic2<br>\n",
    "<br>\n",
    "Где каждая тема сформирована из слов (в порядке убывания значимости):<br>\n",
    "<br>\n",
    "Topic1 = 30% собака, 30% играть, 20% нравиться 10% милая 10% любить<br>\n",
    "Topic2 = 50% кошка, 30% молоко, 20% нравиться<br>\n",
    "\n",
    "\n",
    "\n",
    "# Зачем нужно LDA Topic Modeling\n",
    "\n",
    "Общая задача тематического моделирования - обнаружение скрытой структуры в наборе текстовых данных. \n",
    "Для маленького количества текстов мы могли бы использовать просто tf-idf информацию, но для большого числа документов этого уже не достаточно\n",
    "\n",
    "Практические применения:\n",
    "\n",
    "1. [Суммаризация мнений](https://dl.acm.org/doi/10.1145/1076034.1076161)\n",
    "Управляющим организациям, чьи решения влияют на большие группы людей может быть полезно суммаризовать информацию с выражением мнений из открытых источников (постов и комментариев в соцсетях)\n",
    "\n",
    "2. [Bioinformatics](https://springerplus.springeropen.com/articles/10.1186/s40064-016-3252-8)\n",
    "Применение метода к \"microarray datasets\" - датасетам с последовательностями аминокилот или нуклеотидов - для выявления сткрытой структуры этих данных.\n",
    "\n",
    "3. [Отслеживание трендов в тематиках корпусов текстов](https://timreview.ca/article/1170)\n",
    "Анализ корпуса текстов - научных публикаций на определенную тему - позволяет отследить, как меняется взгляд на ту или иную научную проблему\n",
    "\n",
    "4. [Рекомендательные системы](https://habr.com/ru/company/surfingbird/blog/150607/)\n",
    "Если представить пользователя как набор описания продуктов, которые ему понравились, то можно находить тематики, интересующие пользователя и делать рекомендации в соответствии с ними.\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "# Немного теории\n",
    "В latent diriclet allocaton (LDA) модели каждый документ в корпусе представляется в виде набора тем в соответствии с распределением Дирихле.\n",
    "\n",
    "##  Постановка проблемы\n",
    "- Коллекция документов $D$\n",
    "- Каждый документ $d$ из коллекции представлен набором слов $W_{d} = (w_{1}, ..., w_{n_{d}})$ из словаря $W$, где $n_{d}$ - длина документа $d$\n",
    "- Каждому документу соответствует набор тем\n",
    "- Порядком слов в документе пренебрегают: каждый документ рассматривается как bag-of-words\n",
    "- Каждая тема $t\\in T$ (где $T$ - набор тем) описывается распределением Дирихле $p(w|t)$ на наборе слов $w\\in W$, то есть тема представлена в виде вектора $\\phi_{t} = (p(w|t):w \\in W)$\n",
    "- Каждый документ $d\\in D$ описывается распределением Дирихле $p(t|d)$ на наборе тем $t\\in T$. То есть документ описывается вектором $\\theta_{d} = (p(t|d):t \\in T)$\n",
    "<br>\n",
    "\n",
    "![](https://editor.analyticsvidhya.com/uploads/26864dtm.JPG)\n",
    "\n",
    "Вероятность \"возникновения\" пары \"документ-слово\" можно записать следующим образом:\n",
    "\n",
    "$$\n",
    "p(w|d)=\\sum\\limits_{t\\in T}p(w|t)p(t|d)\n",
    "$$\n",
    "\n",
    "![](https://miro.medium.com/max/780/1*QiTvyHNwvGI5UCqeKvhNsg.png)\n",
    "\n",
    "## Решение\n",
    "Построить модель тематического моделирования значит найти матрицы $\\Phi = ||p(w|t)||$ и $\\Theta = ||p(t|d)||$ на основе коллекции документов $D$.\n",
    "\n",
    "\n",
    "Для нахождения решения нужно ршеить оптимизационную задачу - максимизировать следующую функцию (максимизируем likelihood наших данных с такими матрицами): \n",
    "$$\n",
    "\\sum\\limits_{d\\in D}\\sum\\limits_{w\\in d}n_{dw}logp(w|d)\\to\\max\\limits_{\\Phi,\\Theta},\n",
    "$$\n",
    "где $n_{dw}$ - частота слова $w$ в документе $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import json\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "import pymorphy2\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702af1f",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "DATA_FILE = \"lenta-ru-news.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ebc744",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_data = pd.read_csv(os.path.join(DATA_DIR, DATA_FILE)).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61aa883",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19243c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_data['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b68110",
   "metadata": {},
   "source": [
    "## Чистка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8156f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    rusdata_df = df[(df['topic']!='Библиотека')&(df['topic']!='Бывший СССР')&(df['topic']!='69-я параллель')].reset_index(drop=True).sample(10000, random_state = 42)\n",
    "    rusdata = rusdata_df['text']\n",
    "    return rusdata_df, rusdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b92f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rusdata_df, rusdata = clean_data(rus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rusdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f7fec",
   "metadata": {},
   "source": [
    "# Обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf5fd9",
   "metadata": {},
   "source": [
    "1. Удаление стоп-слов\n",
    "2. Токенизация\n",
    "2. Лемматизация\n",
    "3. Построение словаря n-грамм\n",
    "4. Tf-idf кодирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb9261",
   "metadata": {},
   "source": [
    " ## Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ccfe31",
   "metadata": {},
   "source": [
    "1. Стоп-слова из библиотеки nltk\n",
    "2. Дополнительный набор стоп-слов №1 ([github](https://github.com/stopwords-iso/stopwords-ru))\n",
    "3. Дополнительный набор стоп-слов №2 ([github](https://github.com/Alir3z4/stop-words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_DIR = \"stopwords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e131a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words():\n",
    "    stopwordsrus = list(stopwords.words('russian'))\n",
    "    with open(os.path.join(STOPWORDS_DIR, \"russian.txt\")) as f:\n",
    "        stop_words_1 = f.read().split(\"\\n\")\n",
    "    with open(os.path.join(STOPWORDS_DIR, \"stopwords-ru.txt\")) as f:\n",
    "        stop_words_2 = f.read().split(\"\\n\")\n",
    "    return list(set(stopwordsrus+stop_words_1+stop_words_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwordsru = get_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e0d6f",
   "metadata": {},
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    return list(t.lower() for t in word_tokenize(text) if t.isalpha() and t.lower() not in stopwordsru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "data = [process(t) for t in rusdata]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc68b25",
   "metadata": {},
   "source": [
    "## Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd842492",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def lemmatizer(texts):\n",
    "    return [[morph.parse(word)[0] for word in text] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_data = lemmatizer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a0926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lemma(texts):\n",
    "    norm = []\n",
    "    for t in texts:\n",
    "        res = []\n",
    "        for word in t:\n",
    "            n = word.normal_form\n",
    "            res.append(n)\n",
    "        norm.append(res)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af219018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our lemmatized data ready to be used further:\n",
    "data_norm = extract_lemma(morph_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fa6fdb",
   "metadata": {},
   "source": [
    "# Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3787bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_visualization(list_of_tokenized_sentences):\n",
    "    # Join the different processed titles together.\n",
    "    long_string = ','.join([','.join(x) for x in list_of_tokenized_sentences])\n",
    "\n",
    "    # Create a WordCloud object\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "    # Generate a word cloud\n",
    "    wordcloud.generate(long_string)\n",
    "    # Visualize the word cloud\n",
    "    return wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ceb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_visualization(data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d65204",
   "metadata": {},
   "source": [
    "## Построение n-грамм"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fde91e",
   "metadata": {},
   "source": [
    "*Смотри документацию [Phrases](https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases) и [Phraser](https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.FrozenPhrases). Phraser ускоряет работу Phrases*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10cbd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_norm, min_count=5, threshold=100) \n",
    "trigram = gensim.models.Phrases(bigram[data_norm], threshold=100)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "data_words_trigrams = make_trigrams(data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02886a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_words_trigrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8050b9",
   "metadata": {},
   "source": [
    "## Создание словаря n-gram и BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19329a0",
   "metadata": {},
   "source": [
    "*Смотри документацию [Dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary) и [doc2bow](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(data_words_trigrams)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in data_words_trigrams]\n",
    "# We will also try to filter unimportant words by their tf-idf score, so let's create the tf-idf scores here too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd47c43",
   "metadata": {},
   "source": [
    "## Encoding Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c55aa",
   "metadata": {},
   "source": [
    "TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)\n",
    "\n",
    "[Статья с хорошим описанием](https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089)\n",
    "\n",
    "$t$ — term (word)<br>\n",
    "$d$ — document (set of words)<br>\n",
    "$D$ - коллекция документов<br>\n",
    "$N$ — размер коллекции документов $D$<br>\n",
    "\n",
    "\n",
    "$$tf(t,d) = \\frac{n_{td}}{n_{d}}$$\n",
    "Частота встречаемости слова в документе.<br>\n",
    "$n_{td}$ - количество встречаний слова $t$ в документе $d$, $n_{d}$ - количество слов в документе $d$<br>\n",
    "<br>\n",
    "$$df(t) = \\sum_d{n_{td}}$$\n",
    "Встречаемость слова в корпусе.<br>\n",
    "$n_{td}$ - количество встречаний слова $t$ в документе $d$, $n_{d}$ - количество слов в документе $d$<br>\n",
    "<br>\n",
    "$$idf(t) = log(\\frac{N}{df(t)+1})$$\n",
    "Обратная встречаемость слова в корпусе.<br>\n",
    "<br>\n",
    "\n",
    "**$$tfidf(t,d) = tf(t,d)*idf(t)$$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe49a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf55943",
   "metadata": {},
   "source": [
    "# Построение модели LDA Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951fabc",
   "metadata": {},
   "source": [
    "## Использование корпуса без TF-IDF фильтрации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=80, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec70bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    sent_topics_df = pd.DataFrame (index=range(10000), columns = ['Dominant_Topic1', 'Dominant_Topic2', '%Topic_Contribution1', '%Topic_Contribution2', 'Topic_Keywords1', 'Topic_Keywords2'])\n",
    "    \n",
    "    # Get main topic in each document\n",
    "    for i, text in enumerate(ldamodel[corpus]):\n",
    "        text = text[0]\n",
    "        text = sorted(text, key=lambda x: x[1], reverse=True) #sort % contributions of topic  \n",
    "        # Get the Dominant topic, % of topic contribution and Keywords for each document\n",
    "        for j, (topic_num, topic_contrib) in enumerate(text):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df.Dominant_Topic1[i] = int(topic_num)\n",
    "                sent_topics_df['%Topic_Contribution1'][i] = round(topic_contrib,4)\n",
    "                sent_topics_df['Topic_Keywords1'][i] = topic_keywords\n",
    "                \n",
    "                #sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                \n",
    "            elif j == 1:  # => second dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df.Dominant_Topic2[i] = int(topic_num)\n",
    "                sent_topics_df['%Topic_Contribution2'][i] = round(topic_contrib,4)\n",
    "                sent_topics_df['Topic_Keywords2'][i] = topic_keywords\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "    \n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts, name = \"text\")\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=rusdata.values)\n",
    "\n",
    "\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index(drop = True)\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb16a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.loc[2][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6acf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.loc[2][\"Topic_Keywords1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ac551",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv('./results/dominant_topic_no_tfidf_limit.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087aa193",
   "metadata": {},
   "source": [
    "## Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da969bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_news_no_tfidf_limits')\n",
    "\n",
    "LDAvis_prepared = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_news_no_tfidf_limits' +'.html')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e5e41",
   "metadata": {},
   "source": [
    "# Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73c013",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "[Подробное описание](https://towardsdatascience.com/perplexity-in-language-models-87a196019a94)\n",
    "\n",
    "\n",
    "Оценка генеративной языковой модели.\n",
    "\n",
    "Мы хотим, чтобы наша языковая модель приписывала высокие вероятности реальным предложениям и низкие - нереальным. Поэтому чем ниже PP, тем лучше.\n",
    "\n",
    "\n",
    "$PP(W)$ - обратная вероятность тестового корпуса слов, нормированная на его размер\n",
    "\n",
    "$$PP(W) = \\frac1{P(w_1,...,w_N)^{\\frac1N}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6380ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. The lower the better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8913c57",
   "metadata": {},
   "source": [
    "# Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87920e",
   "metadata": {},
   "source": [
    "## Использование корпуса c TF-IDF фильтрацией (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826cdf03",
   "metadata": {},
   "source": [
    "TFIDF коэффициенты выделяют слова, которые являются часто употребимыми (не несут информации о конкретном документе) или те, которые встречаются только в одном документе и не могут показать скрытую связь между документами. \n",
    "\n",
    "В этом задании нужно\n",
    "1. Найти максимальное и минимальное значение tfidf скора\n",
    "\n",
    "*Подсказка*: нужно проитерироваться по всему корпусу слов, посчитать tfidf скор и найти пороговые значения\n",
    "\n",
    "2. Построить дискретный массив tfidf скоров с шагом 0.005 и найти перцентили 10 и 95\n",
    "\n",
    "3. Отфильтровать corpus в filtered_corpus, оставив только слова со скорами, попадающими в диапазон от 10 до 95 перцентилей.\n",
    "4. Построить подель LDA на отфильтрованном корпусе\n",
    "5. Посчитать метрики качества\n",
    "5. Сделать визуализацию тематик\n",
    "6. Посмотреть на темы с наибольшими коэффициентами в датафрейме\n",
    "7. Напишите, какой вывод можно сделать\n",
    "\n",
    "**Комментарий** - можно уменьшать количество  $n_topics$, если вашему компьютеру не хватает вычислительных мощностей.\n",
    "\n",
    "**Комментарий** - если при визуализации вы получили ошибку `TypeError: Object of type complex is not JSON serializable`, добавьте в метод `prepare` аргумент `mds='mmds'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93db4d",
   "metadata": {},
   "source": [
    "## Дополнительное задание (5 баллов)\n",
    "\n",
    "Постройте LDA Topic modelling модель для [датасета постов о политике с Пикабу](https://www.kaggle.com/atomin/pikabu-politic-posts). \n",
    "\n",
    "При построении можете поизменять параметры `alpha` и `beta` для получения более качественной модели.\n",
    "\n",
    "Какие выводы можно сделать на основе полученной модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_min = (0, 1)\n",
    "global_max = (0, 0)\n",
    "for text in corpus:\n",
    "    vec = tfidf[text]\n",
    "    m, M = min(vec, key=lambda p: p[1]), max(vec, key=lambda p: p[1])\n",
    "  \n",
    "    if m[1] < global_min[1]:\n",
    "        global_min = m\n",
    "    \n",
    "    if M[1] > global_max[1]:\n",
    "        global_max = M\n",
    "\n",
    "tf_max = dictionary[global_max[0]]\n",
    "tf_min = dictionary[global_min[0]]\n",
    "print(tf_max, tf_min)\n",
    "tfidf_range = [x / 1000 for x in range(int(global_min[1] * 1000), int(global_max[1] * 1000) + 5, 5)]\n",
    "\n",
    "print(np.percentile(tfidf_range, 95), np.percentile(tfidf_range, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbab17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_value = np.percentile(tfidf_range, 10) \n",
    "high_value = np.percentile(tfidf_range, 95)\n",
    "filter_ids = set()\n",
    "\n",
    "for i in range(0, len(corpus)):\n",
    "    filter_ids.union(set([\n",
    "        word[0] for word in corpus[i]\n",
    "        if word[1] > high_value or word[1] < low_value\n",
    "    ]))\n",
    "\n",
    "filtered_corpus = []\n",
    "for i in range(0, len(corpus)):\n",
    "    new_bow = [word for word in corpus[i] if word[0] not in filter_ids]\n",
    "    filtered_corpus.append(new_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97cda72",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.ldamodel.LdaModel(corpus=filtered_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=80, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b931bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "pprint(lda_model_tfidf.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612608f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_news_with_tfidf')\n",
    "\n",
    "LDAvis_prepared = gensimvis.prepare(lda_model_tfidf, filtered_corpus, dictionary)\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_with_no_tfidf' +'.html')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8720f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(filtered_corpus))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4bba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model_tfidf, corpus=filtered_corpus, texts=rusdata.values)\n",
    "\n",
    "\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index(drop = True)\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e57de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv('./results/dominant_topic_with_tfidf_limit.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.loc[2][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104049f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dominant_topic.loc[2][\"Topic_Keywords2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb397b",
   "metadata": {},
   "source": [
    "Принципиально ничего не поменялось. Если смотреть на перентили скоров, то они почти равны десяти и девяносто пяти соответственно. Сильно отличается только аутпут последней ячейки. Всё остальное видимо не поменялось, да и время работы примерно то же самое"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
